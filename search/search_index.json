{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"transcriber","text":""},{"location":"#description","title":"Description","text":"<p>This <code>transcribe</code> script is intended to be used, primarily, with the Bonsai Tutorials video collection to create SRT subtitle text files for the tutorial videos.</p> <p>Since there were so many videos it became difficult to remember which one contained a valuable explanation and exactly when in the video that explanation occurred.</p> <p>I found the OpenAI Whisper python library and set about using it to transcribe the videos to SRT subtitle text files (thanks to the pydub and pysrt modules).</p> <p>SRT subtitle text files are described by this Wikipedia entry:</p> <ul> <li>SubRip</li> </ul> <p>I used the cookiecutter-uv project template to try to ensure that I had the basis of a modern python project.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>You can either:</p> <ol> <li>Use python's uv and <code>make</code> to manage the script's dependencies.    See the \ud83d\ude80 Getting Started document for more details.</li> <li>Use the included <code>Dockerfile</code> and the <code>docker</code> command to make use of this script.    See the \ud83d\udea2 Using Docker document for more details.</li> </ol> <p>Notes Each matching video found in the target directory will have an <code>.srt</code> file created as a sibling. This makes it easy for video players to match the video with the subtitles.</p>"},{"location":"docker/","title":"Using Docker","text":"<p>Work in Progress</p>"},{"location":"docker/#using-docker-to-run-transcribe","title":"Using Docker to Run Transcribe","text":"<p>The beauty of using <code>docker</code> to run our <code>transcribe</code> script is that it handles all the dependencies for you.</p> <p>All you need is to have <code>docker</code> installed on your system and the <code>Dockerfile</code> found at the top-level of this repository.</p> <p>You can then do the following to transcribe your video files to SRT subtitle text files:</p> <pre><code>$ cd ~/projects/transcribe # Change directory to the directory which contains the Dockerfile.\n$ docker run -v ~/projects/Bonsai_Tutorials:/tutorials\n</code></pre> <p>Work in Progress</p>"},{"location":"getting-started/","title":"\ud83d\ude80 Getting Started","text":""},{"location":"getting-started/#getting-started","title":"Getting Started","text":"<p>This project uses <code>uv</code> for dependency management and <code>pyenv</code> to ensure you're using the correct Python version. Follow the steps below for your operating system to quickly set up the environment and start developing.</p>"},{"location":"getting-started/#1-install-system-prerequisites","title":"1. Install System Prerequisites","text":"<p>First, you need to install <code>direnv</code>, <code>pyenv</code>, <code>uv</code>, and <code>ffmpeg</code>.</p>"},{"location":"getting-started/#macos","title":"\ud83c\udf4e macOS","text":"<p>On MacOS, the easiest way to install these tools is using Homebrew. If you don't have Homebrew, install it first by following the instructions on the Homebrew website.</p> <pre><code># Install direnv for managing per directory environment variables\n$ brew install direnv\n\n# Install pyenv for managing Python versions\n$ brew install pyenv\n\n# Install uv (the package manager)\n$ brew install uv\n\n# Install FFmpeg for audio/video processing\n$ brew install ffmpeg\n</code></pre> <p>Note: You must complete the <code>pyenv</code> shell configuration steps printed after installation (e.g., adding <code>eval \"$(pyenv init -)\"</code> to your shell config file like <code>~/.zshrc</code>).</p> <p>Note: You must use <code>\"direnv allow\"</code> when you change into the transcribe directory so that it can set the environment variables it needs to using the <code>.envrc</code> file.</p>"},{"location":"getting-started/#ubuntu-and-wsl2","title":"\ud83d\udc27 Ubuntu (and WSL2)","text":"<p>Install the prerequisites using the Advanced Package Tool (<code>apt</code>).</p> <pre><code>$ sudo apt update\n$ sudo apt install -y make build-essential libssl-dev zlib1g-dev \\\\\n           libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \\\\\n           libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev\n\n# Install pyenv, uv, and ffmpeg\n$ curl https://pyenv.run | bash\n$ curl -LsSf https://astral.sh/uv/install.sh | bash\n$ sudo apt install -y direnv\n$ sudo apt install -y ffmpeg\n</code></pre> <p>Note: After running the <code>pyenv</code> installation script, you must add the initialization commands (printed to your console) to your shell's configuration file (<code>~/.bashrc</code> or <code>~/.zshrc</code>) and then restart your terminal.</p> <p>Note: You must use <code>\"direnv allow\"</code> when you change into the transcribe directory so that it can set the environment variables it needs to using the <code>.envrc</code> file.</p>"},{"location":"getting-started/#windows","title":"\ud83e\ude9f Windows","text":"<p>For a smooth development experience, the recommended approach is to use the Windows Subsystem for Linux (WSL2) and follow the Ubuntu instructions above.</p>"},{"location":"getting-started/#2-project-setup","title":"2. Project Setup","text":"<p>The project uses a <code>Makefile</code> to simplify setup. Once your prerequisites are installed, you can set up the entire environment with a single command.</p>"},{"location":"getting-started/#step-21-install-python-and-create-the-environment","title":"Step 2.1: Install Python and Create the Environment","text":"<p>The following command will automatically: 1.  Use <code>pyenv</code> to install the required Python version. 2.  Use <code>uv</code> to create the virtual environment (<code>.venv</code>). 3.  Use <code>uv</code> to install all project dependencies. 4.  Install <code>pre-commit</code> hooks.</p> <pre><code># This command runs `pyenv install` and then `uv sync`\n$ make install\n</code></pre>"},{"location":"getting-started/#step-22-activate-the-environment","title":"Step 2.2: Activate the Environment","text":"<p>You can manually activate the virtual environment every time you open a new terminal to work on the project however, <code>direnv</code> should do this for you automatically based on our <code>.envrc</code> file.</p> <p>Here are the commands to activate your virtual python environment manually.</p> <pre><code># For macOS/Ubuntu/WSL\n$ source .venv/bin/activate\n# For native Windows (cmd or PowerShell)\n# .venv\\\\Scripts\\\\activate\n</code></pre>"},{"location":"getting-started/#3-make-commands","title":"3. Make Commands","text":"<p>The project uses <code>make</code> targets to standardize common tasks. Ensure your virtual environment is active before running these commands.</p> Command Description <code>make install</code> Setup: Install the virtual environment, dependencies, and pre-commit hooks. <code>make check</code> Quality: Run code quality tools (linters/formatters, dependency checks, and type-checking). <code>make test</code> Quality: Run the automated tests using <code>pytest</code>. <code>make docs-test</code> Quality: Test if documentation can be built without warnings or errors. <code>make build</code> Distribution: Build the distributable wheel (<code>.whl</code>) and source archive (<code>.tar.gz</code>) files in the <code>dist/</code> directory. <code>make clean-build</code> Cleanup: Remove build artifacts from the <code>dist/</code> directory. <code>make docs</code> Documentation: Build the documentation and serve it locally (usually at <code>http://127.0.0.1:8000</code>). <code>make man</code> Documentation: Display the <code>transcribe</code> scripts <code>--help</code> output in the form of a <code>man</code> page. <code>make transcribe</code> Execution: Interactively run the main application to transcribe video files to SRT subtitle files. <code>make help</code> Help: Display this help message with descriptions for all targets."},{"location":"getting-started/#31-reading-the-command-man-page","title":"3.1. Reading the Command \"man\" page.","text":"<p>To see the options that you can give the <code>transcribe.py</code> script simple run the following:</p> <pre><code># 1. Run the \"man\" target.\n$ make man\n</code></pre> <p>This will display the following \"man\" page:</p> <pre><code>$ make man\nusage: transcribe.py [-h] [--dry-run] [--include [INCLUDE ...]] [--exclude [EXCLUDE ...]] [--force] [--input-path INPUT_PATH] [--suffix SUFFIX] [--model {tiny.en,base.en,small.en,medium.en}] [--interactive] [--version]\n\nTranscribe audio files using a pre-trained model.\n\noptions:\n  -h, --help            show this help message and exit\n  --dry-run, -n         Try a dry run without any actual transcription.\n  --include [INCLUDE ...]\n                        A list of files or rglob patterns to include when processing. Defaults to **/*.mp4.\n  --exclude [EXCLUDE ...]\n                        A list of files or rglob patterns to exclude from processing (overrides the include list).\n  --force               Force overwrite of existing output SRT files.\n  --input-path INPUT_PATH\n                        Directory containing input audio files (required in non-interactive mode).\n  --suffix SUFFIX       Suffix of audio files to process (default: .mp4).\n  --model {tiny.en,base.en,small.en,medium.en}\n                        Pre-trained model to use (default: base.en, available ['tiny.en', 'base.en', 'small.en', 'medium.en']).\n  --interactive         Run in interactive mode, prompting for missing arguments.\n  --version, -v         Show program's version number and exit.\n</code></pre> <p>Giving no arguments will cause the command to run in \"interactive\" mode, prompting you for any information you need. Supplying the arguments on the command-line gives you more control. You can override the interactive mode's defaults by supplying the <code>--interactive</code> option combined with the command options you want to override.</p>"},{"location":"getting-started/#32-running-the-transcriber-using-make","title":"3.2. Running the Transcriber using \"make\"","text":"<p>To run the main functionality of the project, use the dedicated \"transcribe\" target:</p> <p><pre><code># 1. Run the interactive transcriber tool\n$ make transcribe\n</code></pre> This will display the current transcription defaults and prompt you interactively for any defaults you want to change. See the next section for more details and an example.</p>"},{"location":"getting-started/#321-interacting-with-the-transcriber","title":"3.2.1 Interacting with the Transcriber","text":"<p>The <code>Makefile</code> transcribe target actually calls our script with the following arguments The <code>uv run</code> part ensures that we are running under the correct python environment:</p> <pre><code>transcribe:\n    @uv run python src/transcriber/transcribe.py --interactive --exclude \\\n    \"_Model/sheets/jpgs/output.mp4\" \\\n    \"_Model/OD_Textures/Open Source/AmbientCG/space-generation-success.mp4\" \\\n    \"_Model/OD_Textures/Open Source/AmbientCG/space-generation-fail.mp4\" \\\n    \"_Model/Animation/final video.mp4\"\n</code></pre> <p>This <code>Makefile</code> target causes the script to run interactively but also excludes some of the mp4 videos, typically found in the Bonsai_Tutorials directory, which have no audio to transcribe.</p> <p>In interactive mode the script displays the current settings and then gives you the opportunity to override some of these defaults (the non-interactive mode lets you override anything with command-line options).</p> <p>Here is my attempt to use the <code>Makefile</code> to run our script and tweak it to use some non-defaults i.e.</p> <ul> <li>my preferred path (<code>~/projects/Bonsai_Tutorials</code>)</li> <li>the largest Whisper model available (<code>medium.en</code> instead of the default smallest model,<code>base.en</code>).</li> </ul> <p>Okay, let's run <code>make transcribe</code>:</p> <pre><code>$ make transcribe\nEntering interactive mode. Please provide the required information.\nEnter the directory with videos (default: .):\n</code></pre> <p>The first thing the script needs to know is where the video files live. We do not want the default (the current working directory) so we'll enter the location of our videos as being <code>~/projects/Bonsai_Tutorials</code>:</p> <pre><code>Enter the directory with videos (default: .): ~/projects/Bonsai_\n</code></pre> <p>The script then shows the current default settings and prompts you to see if you want to override the current defaults:</p> <p><pre><code>Current settings for transcribe version 1.0.0:\n  Suffix: .mp4\n  Model: base.en\n  Force overwrite: No\n  Dry run: No\n  Excluded patterns: (_Model/sheets/jpgs/output.mp4, _Model/OD_Textures/Open Source/AmbientCG/space-generation-success.srt, _Model/OD_Textures/Open Source/AmbientCG/space-generation-fail.srt, _Model/Animation/final video.srt)\n  Include patterns: (None)\nYou will now be prompted for any changes to these settings.\nEnter suffix to process (or press Enter to keep '.mp4'):\n</code></pre> We'll take the default <code>suffix</code> by hitting the <code>Enter</code> (or <code>Return</code>) key.</p> <p>Next, we are prompted to enter the Whisper model we want to use, the default is the smallest (<code>base.en</code>), which gives great results, but we will go with the largest model instead (<code>medium,en</code>). To be honest, the smallest model's results are pretty similar.</p> <p>We'll take the default for \"Force overwrite\" (No), this stops the script overwriting any existing <code>.srt</code> files). We'll also take the default for \"Enable dry run mode\" (No), this means the script will actually perform the transcription instead of just printing what it would have done.</p> <p><pre><code>Enter model to use (or press Enter to keep 'base.en', available tiny.en, base.en, small.en, medium.en): medium.en\nForce overwrite of existing SRT files? (y/N, default: N):\nEnable dry run mode? (y/N, default: N):\n</code></pre> At this point, the script will output a summary of your choices and ask you to confirm if you want to proceed.</p> <pre><code>Confirm settings for transcribe version 1.0.0:\n  Suffix: .mp4\n  Model: medium.en\n  Force overwrite: No\n  Dry run: No\n  Excluded patterns: (_Model/sheets/jpgs/output.mp4, _Model/OD_Textures/Open Source/AmbientCG/space-generation-success.srt, _Model/OD_Textures/Open Source/AmbientCG/space-generation-fail.srt, _Model/Animation/final video.srt)\n  Include patterns: (None)\n\nHit Enter to continue, or Ctrl-C to abort.\n</code></pre> <p>The script waits for you to confirm either by hitting the <code>Enter</code> (or <code>Return</code>) key or cancel by hitting <code>Ctrl-C</code>.</p> <p>We hit <code>Enter</code> and, as the CPU or GPU starts to glow white hot, we eventually get SRT subtitle text files as siblings to all the <code>.mp4</code> files we recursively found in our <code>Bonsai_Tutorials</code>.</p> <p>You should see quite a bit of output as the script processes each video.</p>"},{"location":"getting-started/#example-of-how-i-use-the-srt-files","title":"Example of How I use the SRT files.","text":"<p>To find a critical explanation, now that I have all the transcriptions available as <code>.srt</code> SRT subtitle text files, I can simply use any text searching tool I want to find the topic I'm interested in:</p> <pre><code>$ find Bonsai_Tutorials -name \\*.srt -exec grep -i 'profiles' {} \\; -print\nGo down to our profiles\nBonsai_Tutorials/077000_20250303_1601 - Working with Arrays/077000_20250303_1601 - Working with Arrays.srt\nAnd I'm going to create a custom profile from that. So go down to our profiles and click on this object to pick it up.\nBonsai_Tutorials/077000_20250303_1601 - Working with Arrays/077000_20250303_1601 - Working with Arrays.base.srt\nalso you can purge unused profiles and sorry unused types as well but I'm not\nBonsai_Tutorials/113000_20250418_1626 - Purging unused materials and styles from the file/113000_20250418_1626 - Purging unused materials and styles from the file.srt\nAnd there's also you can purge unused profiles and sorry unused types as well.\nBonsai_Tutorials/113000_20250418_1626 - Purging unused materials and styles from the file/113000_20250418_1626 - Purging unused materials and styles from the file.base.srt\nTo those profiles and layer sets.\nBonsai_Tutorials/093000_20250312_1635 - Annotation tag types/093000_20250312_1635 - Annotation tag types.base.srt\nmaterial was already signed to that to those profiles and layer sets.\nBonsai_Tutorials/093000_20250312_1635 - Annotation tag types/093000_20250312_1635 - Annotation tag types.srt\nClick on this dropdown, you can see all the different types of profiles that IFC offers.\nBonsai_Tutorials/069000_20250226_1738 - Adding strip footings/069000_20250226_1738 - Adding strip footings.base.srt\nClick on this drop down you can see all the different types of profiles that IFC offers\nBonsai_Tutorials/069000_20250226_1738 - Adding strip footings/069000_20250226_1738 - Adding strip footings.srt\nYou can pull in, you know, I've seen materials, profiles, and types.\nBonsai_Tutorials/080000_20250304_1723 - pulling in content or assets from other files/080000_20250304_1723 - pulling in content or assets from other files.srt\nAnd you can pull in, I've seen materials, profiles and types.\nBonsai_Tutorials/080000_20250304_1723 - pulling in content or assets from other files/080000_20250304_1723 - pulling in content or assets from other files.base.srt\nWe're going to go to profiles and we're going to use this arbitrary, closed profile\nBonsai_Tutorials/070000_20250227_0930 - Thickened edge with custom profile/070000_20250227_0930 - Thickened edge with custom profile.base.srt\nWe're going to go to profiles and we're going to use this arbitrary closed profile def.\nBonsai_Tutorials/070000_20250227_0930 - Thickened edge with custom profile/070000_20250227_0930 - Thickened edge with custom profile.srt\nThe cabinets here are kind of just generic, massing cabinets. They're actually extruded profiles\nBonsai_Tutorials/124000_20250522_1549 - Intro to Git and creating a floor outline with surrounding walls/124000_20250522_1549 - Intro to Git and creating a floor outline with surrounding walls.base.srt\nThey're actually extruded profiles if I tab into them.\nBonsai_Tutorials/124000_20250522_1549 - Intro to Git and creating a floor outline with surrounding walls/124000_20250522_1549 - Intro to Git and creating a floor outline with surrounding walls.srt\n</code></pre> <p>Armed with these clues, I can then use <code>vlc</code>, or some equivalent video player, and I can find the exact section of the video I need to watch.</p> <p>You can also use the timings in the SRT file to find the time the phrase occurred.</p> <p>Any advice for improvement much appreciated...</p> <p>AtDhVaAnNkCsE</p> <p>Doug Scoular dscoular@gmail.com 2025/10/23</p>"},{"location":"modules/","title":"Module Documentation","text":"<p>Apologies for the sometimes poor formatting of text, it is machine generated by mkdocstrings.</p> <p>Cheers</p> <p>Doug Scoular</p> <p>Transcribe video files to SRT subtitle text files using a pre-trained model.</p> <p>Author: Doug Scoular Date:   2025-09-16 Email:  dscoular@gmail.com</p> <p>License: MIT</p> <p>The main class that does all the work is Transcribe it employs an instance of the FileFilter class to find the video files it is going to process and turn into SRT subtitle text files.</p> <p>Requirements: (see pyproject.toml for versions):</p> <ul> <li>whisper (openai/whisper)</li> <li>pysrt</li> <li>numpy</li> <li>AudioSegment (pydub)</li> <li>ffmpeg (for audio decoding, must be installed separately into the Operating System)</li> </ul>"},{"location":"modules/#transcriber.transcribe.FileFilter","title":"<code>FileFilter</code>","text":"<p>A class which recursively searches the given input_path filtering files it finds based on a matching filename \"suffix\" e.g. .mp4 combined with \"include\" and \"exclude\" rglob patterns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Instantiating our FileFilter instance and obtaining matching files.\n&gt;&gt;&gt; filter = FileFilter(Path('.'),\n...                     '.mp4',\n...                     include_patterns=['**/*.mp4'],\n...                     exclude_patterns=['**/exclude_this.mp4'])\n&gt;&gt;&gt; matching_files = filter.get_matching_files()\n&gt;&gt;&gt; for file in matching_files:\n&gt;&gt;&gt;     print(file)\nfoo.mp4\nbar.mp4\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Path</code> <p>The root directory to scan for files.</p> required <code>suffix</code> <code>Optional[str]</code> <p>The file suffix to filter by (defaults to '.mp4').</p> <code>'.mp4'</code> <code>include_patterns</code> <code>Optional[list[str]]</code> <p>List of glob patterns to include.</p> <code>None</code> <code>exclude_patterns</code> <code>Optional[list[str]]</code> <p>List of glob patterns to exclude.</p> <code>None</code> Source code in <code>src/transcriber/transcribe.py</code> <pre><code>class FileFilter:\n    \"\"\"\n    A class which recursively searches the given input_path filtering files it finds\n    based on a matching filename \"suffix\" e.g. .mp4 combined with \"include\" and \"exclude\"\n    rglob patterns.\n\n    Examples:\n        &gt;&gt;&gt; # Instantiating our FileFilter instance and obtaining matching files.\n        &gt;&gt;&gt; filter = FileFilter(Path('.'),\n        ...                     '.mp4',\n        ...                     include_patterns=['**/*.mp4'],\n        ...                     exclude_patterns=['**/exclude_this.mp4'])\n        &gt;&gt;&gt; matching_files = filter.get_matching_files()\n        &gt;&gt;&gt; for file in matching_files:\n        &gt;&gt;&gt;     print(file)\n        foo.mp4\n        bar.mp4\n\n    Args:\n        input_path (Path): The root directory to scan for files.\n        suffix (Optional[str]): The file suffix to filter by (defaults to '.mp4').\n        include_patterns (Optional[list[str]]): List of glob patterns to include.\n        exclude_patterns (Optional[list[str]]): List of glob patterns to exclude.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_path: Path,\n        suffix: str | None = \".mp4\",\n        include_patterns: list[str] | None = None,\n        exclude_patterns: list[str] | None = None,\n    ):\n        self.input_path = input_path.resolve()\n        self.suffix = suffix\n        # If the user provides no include patterns, the default is to find\n        # all files with the given suffix, recursively.\n        # First, remove any empty, duplicate patterns. Our argument parsing should have already handled this,\n        self.include_patterns = sorted({pattern for pattern in include_patterns or [] if pattern})\n        self.exclude_patterns = sorted({pattern for pattern in exclude_patterns or [] if pattern})\n        # Use the default include pattern if none provided.\n        self.include_patterns = include_patterns or [f\"**/*{self.suffix}\"]\n        # If no exclude patterns are provided, default to an empty list.\n        self.exclude_patterns = exclude_patterns or []\n\n    def get_matching_files(self) -&gt; list[Path]:\n        \"\"\"\n        Recursively Scans the self.input_path directory using\n        rglob patterns and returns a list of all files that match our\n        FileFilter instance's criteria (suffix, include_patterns and\n        exclude_patterns).\n\n        Returns:\n            A sorted list of Path objects matching the filter criteria.\n        \"\"\"\n        included_files: set[Path] = set()\n        for pattern in self.include_patterns:\n            # Path.glob with '**' handles recursive search automatically.\n            # This correctly interprets patterns like '**/*.mkv'.\n            for file in self.input_path.glob(pattern):\n                if file.is_file():\n                    included_files.add(file)\n\n        excluded_files: set[Path] = set()\n        for pattern in self.exclude_patterns:\n            for file in self.input_path.glob(pattern):\n                if file.is_file():\n                    excluded_files.add(file)\n\n        # The final set of files is the difference between the two sets.\n        matching_files = sorted(included_files - excluded_files)\n\n        print(f\"We matched {len(matching_files)} files.\")\n        if excluded_files:\n            print(\"The following files were explicitly excluded by your exclude rules:\")\n            for excluded_file in sorted(excluded_files):\n                print(f\"  EXCLUDED: [{excluded_file}]\")\n\n        return matching_files\n</code></pre>"},{"location":"modules/#transcriber.transcribe.FileFilter.get_matching_files","title":"<code>get_matching_files()</code>","text":"<p>Recursively Scans the self.input_path directory using rglob patterns and returns a list of all files that match our FileFilter instance's criteria (suffix, include_patterns and exclude_patterns).</p> <p>Returns:</p> Type Description <code>list[Path]</code> <p>A sorted list of Path objects matching the filter criteria.</p> Source code in <code>src/transcriber/transcribe.py</code> <pre><code>def get_matching_files(self) -&gt; list[Path]:\n    \"\"\"\n    Recursively Scans the self.input_path directory using\n    rglob patterns and returns a list of all files that match our\n    FileFilter instance's criteria (suffix, include_patterns and\n    exclude_patterns).\n\n    Returns:\n        A sorted list of Path objects matching the filter criteria.\n    \"\"\"\n    included_files: set[Path] = set()\n    for pattern in self.include_patterns:\n        # Path.glob with '**' handles recursive search automatically.\n        # This correctly interprets patterns like '**/*.mkv'.\n        for file in self.input_path.glob(pattern):\n            if file.is_file():\n                included_files.add(file)\n\n    excluded_files: set[Path] = set()\n    for pattern in self.exclude_patterns:\n        for file in self.input_path.glob(pattern):\n            if file.is_file():\n                excluded_files.add(file)\n\n    # The final set of files is the difference between the two sets.\n    matching_files = sorted(included_files - excluded_files)\n\n    print(f\"We matched {len(matching_files)} files.\")\n    if excluded_files:\n        print(\"The following files were explicitly excluded by your exclude rules:\")\n        for excluded_file in sorted(excluded_files):\n            print(f\"  EXCLUDED: [{excluded_file}]\")\n\n    return matching_files\n</code></pre>"},{"location":"modules/#transcriber.transcribe.Transcriber","title":"<code>Transcriber</code>","text":"<p>A class to handle transcription of video files to SRT subtitle text files using an OpenAI/Whisper pre-trained model. Takes our parsed command-line arguments to instantiate an instance which we can then use to transcribe videos to text.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Manually create our arguments namespace.\n&gt;&gt;&gt; my_args = argparse.Namespace(input_path='/tmp/Bonsai_Tutorials')\n&gt;&gt;&gt; my_args.model = 'base.en' # Choose the smallest transcription model.\n&gt;&gt;&gt; my_args.force = True  # Force overwriting existing \".srt\" files.\n&gt;&gt;&gt; my_args.suffix = '.mp4'  # Only consider \".mp4\" files.\n&gt;&gt;&gt; # Include \"rglob\" patterns we are interested in.\n&gt;&gt;&gt; my_args.include = ['**/001000_20250218_1337 - moving objects and setting a few preferences.mp4']\n&gt;&gt;&gt; # Exclude \"rglob\" patterns we don't want to process.\n&gt;&gt;&gt; my_args.excluded = ['**/skip_this.mp4']\n&gt;&gt;&gt; my_args.dry_run = False  # Actually process the files.\n&gt;&gt;&gt; my_args.interactive = False #  Don't interactively prompt the user.\n&gt;&gt;&gt; # Instantiate our Transcriber instance with our arguments.\n&gt;&gt;&gt; transcriber = Transcriber(my_args)\n&gt;&gt;&gt; # Start the transcription process.\n&gt;&gt;&gt; transcriber.videos_to_text()\nWe matched 1 files.\nPROCESSING: /tmp/Bonsai_Tutorials/001000_20250218_1337 - moving objects and setting a few preferences/001000_20250218_1337 - moving objects and setting a few preferences.mp4 -&gt; /tmp/Bonsai_Tutorials/001000_20250218_1337 - moving objects and setting a few preferences/001000_20250218_1337 - moving objects and setting a few preferences.srt...\nSUCCESS: Transcription saved to [/tmp/Bonsai_Tutorials/001000_20250218_1337 - moving objects and setting a few preferences/001000_20250218_1337 - moving objects and setting a few preferences.srt]\nTranscription completed for all files.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required Source code in <code>src/transcriber/transcribe.py</code> <pre><code>class Transcriber:\n    \"\"\"\n    A class to handle transcription of video files to SRT subtitle text files\n    using an OpenAI/Whisper pre-trained model. Takes our parsed command-line\n    arguments to instantiate an instance which we can then use to\n    transcribe videos to text.\n\n    Examples:\n        &gt;&gt;&gt; # Manually create our arguments namespace.\n        &gt;&gt;&gt; my_args = argparse.Namespace(input_path='/tmp/Bonsai_Tutorials')\n        &gt;&gt;&gt; my_args.model = 'base.en' # Choose the smallest transcription model.\n        &gt;&gt;&gt; my_args.force = True  # Force overwriting existing \".srt\" files.\n        &gt;&gt;&gt; my_args.suffix = '.mp4'  # Only consider \".mp4\" files.\n        &gt;&gt;&gt; # Include \"rglob\" patterns we are interested in.\n        &gt;&gt;&gt; my_args.include = ['**/001000_20250218_1337 - moving objects and setting a few preferences.mp4']\n        &gt;&gt;&gt; # Exclude \"rglob\" patterns we don't want to process.\n        &gt;&gt;&gt; my_args.excluded = ['**/skip_this.mp4']\n        &gt;&gt;&gt; my_args.dry_run = False  # Actually process the files.\n        &gt;&gt;&gt; my_args.interactive = False #  Don't interactively prompt the user.\n        &gt;&gt;&gt; # Instantiate our Transcriber instance with our arguments.\n        &gt;&gt;&gt; transcriber = Transcriber(my_args)\n        &gt;&gt;&gt; # Start the transcription process.\n        &gt;&gt;&gt; transcriber.videos_to_text()\n        We matched 1 files.\n        PROCESSING: /tmp/Bonsai_Tutorials/001000_20250218_1337 - moving objects and setting a few preferences/001000_20250218_1337 - moving objects and setting a few preferences.mp4 -&gt; /tmp/Bonsai_Tutorials/001000_20250218_1337 - moving objects and setting a few preferences/001000_20250218_1337 - moving objects and setting a few preferences.srt...\n        SUCCESS: Transcription saved to [/tmp/Bonsai_Tutorials/001000_20250218_1337 - moving objects and setting a few preferences/001000_20250218_1337 - moving objects and setting a few preferences.srt]\n        Transcription completed for all files.\n\n    Args:\n        args: Parsed command-line arguments.\n    \"\"\"  # noqa: E501\n\n    # Instance variables with types (Python 3.6+ allows this)\n    force: bool\n    input_path: Path\n    model: Any  # The whisper model type is not explicitly defined.\n    suffix: str\n    filter: FileFilter\n\n    def __init__(self, args: argparse.Namespace) -&gt; None:\n        self.input_path = Path(args.input_path).expanduser()\n        self.force = args.force\n        self.model = args.model\n        self.suffix = args.suffix\n        self.dry_run = args.dry_run\n        self.filter = FileFilter(self.input_path, self.suffix, args.include, args.exclude)\n\n    def transcribe(self, input_file: Path) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Transcribe the audio from the given video input file and returns a dictionary of\n        transcribed text and other relevant metadata. We return None if the transcription fails.\n\n        Examples:\n            &gt;&gt;&gt; # Transcribe our video to our dictionary of subtitle metadata.\n            &gt;&gt;&gt; srt_metadata = transcriber.transcribe(\"/path/to/video.mp4\")\n            &gt;&gt;&gt; pp srt_metadata\n            {\n              'language': 'en',\n              'segments': [{'avg_logprob': -0.18892038023317015,\n               'compression_ratio': 1.5515463917525774,\n               'end': 6.28,\n               'id': 0,\n               'no_speech_prob': 0.09762045741081238,\n               'seek': 0,\n               'start': 0.0,\n               'temperature': 0.0,\n               'text': ' Welcome to Vanilla Blender. I figured before we get into Bonsai we can go'\n               ...\n            }\n\n        Args:\n            input_file: The root directory to scan for files.\n        Returns:\n            A dictionary with a dictionary of transcription results, or None on failure.\n        \"\"\"\n        try:\n            # pydub will internally use ffmpeg if it's available\n            # It will try to decode the MP4 directly.\n            # You might need to specify the format if pydub can't guess from the extension.\n            audio_segment: Any = AudioSegment.from_file(str(input_file))\n\n            # Crucially, ensure the audio is 16kHz, mono\n            # Whisper typically expects 16kHz mono float32\n            audio_segment = audio_segment.set_frame_rate(16000).set_channels(1)\n\n            audio_data: np.ndarray = np.frombuffer(audio_segment.get_array_of_samples(), dtype=np.int16)\n\n            # Convert to float32 and normalize\n            audio_data_float: np.ndarray = audio_data.astype(np.float32) / 32768.0\n\n            # Use whisper's transcribe method to get the transcription model.\n            model = whisper.load_model(self.model)\n            result: dict[str, Any] = model.transcribe(audio_data_float, fp16=False)\n        except (FileNotFoundError, ValueError, TypeError) as e:\n            # Catch known potential errors.\n            print(f\"ERROR: skipping [{input_file}]: {e}\")\n            return None  # Skip this file on known errors.\n        # Return our transcribe() result.\n        return result\n\n    def videos_to_text(self) -&gt; None:\n        \"\"\"\n        Convert video files in the input path to audio and transcribe them to SRT text files\n        based on the arguments given when we instantiated our Transcriber class.\n        \"\"\"\n        # Enumerate our input files.\n        for input_filename in sorted(self.filter.get_matching_files()):\n            if self.dry_run:\n                print(f\"DRY RUN ENABLED, skipping actual transcription of [{input_filename}]\")\n                continue\n            # Are we likely to overwrite an existing .srt file?\n            output_srt_file = input_filename.with_suffix(\".srt\")\n            if not self.force and output_srt_file.exists():\n                print(\n                    f\"SKIPPING: Transcription for [{input_filename}] already exists \"\n                    f\"as [{output_srt_file}] (use --force to overwrite).\"\n                )\n                continue\n\n            print(f\"PROCESSING: {input_filename} -&gt; {output_srt_file}...\")\n            transcription: dict[str, Any] | None = None\n            try:\n                transcription = self.transcribe(input_filename)\n            except IndexError as err:\n                print(f\"ERROR: Skipping [{input_filename}] due to [{err}]\")\n                continue\n            if transcription:\n                # Create a SubRipFile object to hold the subtitles.\n                subs = pysrt.SubRipFile()\n                for i, segment in enumerate(transcription[\"segments\"]):\n                    start_time_ms = int(segment[\"start\"] * 1000)\n                    end_time_ms = int(segment[\"end\"] * 1000)\n                    text = segment[\"text\"].strip()\n\n                    # Create SubRipTime objects.\n                    start_time = pysrt.SubRipTime(milliseconds=start_time_ms)\n                    end_time = pysrt.SubRipTime(milliseconds=end_time_ms)\n\n                    # Create a SubRipItem and add it to the file.\n                    sub = pysrt.SubRipItem(index=i + 1, start=start_time, end=end_time, text=text)\n                    subs.append(sub)\n\n                # Save the SRT file.\n                subs.save(output_srt_file, encoding=\"utf-8\")\n\n                print(f\"SUCCESS: Transcription saved to [{output_srt_file}]\")\n            else:\n                print(f\"ERROR: Empty transcribe() return value: [{input_filename}]\")\n\n        print(\"Transcription completed for all files.\")\n</code></pre>"},{"location":"modules/#transcriber.transcribe.Transcriber.transcribe","title":"<code>transcribe(input_file)</code>","text":"<p>Transcribe the audio from the given video input file and returns a dictionary of transcribed text and other relevant metadata. We return None if the transcription fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Transcribe our video to our dictionary of subtitle metadata.\n&gt;&gt;&gt; srt_metadata = transcriber.transcribe(\"/path/to/video.mp4\")\n&gt;&gt;&gt; pp srt_metadata\n{\n  'language': 'en',\n  'segments': [{'avg_logprob': -0.18892038023317015,\n   'compression_ratio': 1.5515463917525774,\n   'end': 6.28,\n   'id': 0,\n   'no_speech_prob': 0.09762045741081238,\n   'seek': 0,\n   'start': 0.0,\n   'temperature': 0.0,\n   'text': ' Welcome to Vanilla Blender. I figured before we get into Bonsai we can go'\n   ...\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>Path</code> <p>The root directory to scan for files.</p> required <p>Returns:     A dictionary with a dictionary of transcription results, or None on failure.</p> Source code in <code>src/transcriber/transcribe.py</code> <pre><code>def transcribe(self, input_file: Path) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Transcribe the audio from the given video input file and returns a dictionary of\n    transcribed text and other relevant metadata. We return None if the transcription fails.\n\n    Examples:\n        &gt;&gt;&gt; # Transcribe our video to our dictionary of subtitle metadata.\n        &gt;&gt;&gt; srt_metadata = transcriber.transcribe(\"/path/to/video.mp4\")\n        &gt;&gt;&gt; pp srt_metadata\n        {\n          'language': 'en',\n          'segments': [{'avg_logprob': -0.18892038023317015,\n           'compression_ratio': 1.5515463917525774,\n           'end': 6.28,\n           'id': 0,\n           'no_speech_prob': 0.09762045741081238,\n           'seek': 0,\n           'start': 0.0,\n           'temperature': 0.0,\n           'text': ' Welcome to Vanilla Blender. I figured before we get into Bonsai we can go'\n           ...\n        }\n\n    Args:\n        input_file: The root directory to scan for files.\n    Returns:\n        A dictionary with a dictionary of transcription results, or None on failure.\n    \"\"\"\n    try:\n        # pydub will internally use ffmpeg if it's available\n        # It will try to decode the MP4 directly.\n        # You might need to specify the format if pydub can't guess from the extension.\n        audio_segment: Any = AudioSegment.from_file(str(input_file))\n\n        # Crucially, ensure the audio is 16kHz, mono\n        # Whisper typically expects 16kHz mono float32\n        audio_segment = audio_segment.set_frame_rate(16000).set_channels(1)\n\n        audio_data: np.ndarray = np.frombuffer(audio_segment.get_array_of_samples(), dtype=np.int16)\n\n        # Convert to float32 and normalize\n        audio_data_float: np.ndarray = audio_data.astype(np.float32) / 32768.0\n\n        # Use whisper's transcribe method to get the transcription model.\n        model = whisper.load_model(self.model)\n        result: dict[str, Any] = model.transcribe(audio_data_float, fp16=False)\n    except (FileNotFoundError, ValueError, TypeError) as e:\n        # Catch known potential errors.\n        print(f\"ERROR: skipping [{input_file}]: {e}\")\n        return None  # Skip this file on known errors.\n    # Return our transcribe() result.\n    return result\n</code></pre>"},{"location":"modules/#transcriber.transcribe.Transcriber.videos_to_text","title":"<code>videos_to_text()</code>","text":"<p>Convert video files in the input path to audio and transcribe them to SRT text files based on the arguments given when we instantiated our Transcriber class.</p> Source code in <code>src/transcriber/transcribe.py</code> <pre><code>def videos_to_text(self) -&gt; None:\n    \"\"\"\n    Convert video files in the input path to audio and transcribe them to SRT text files\n    based on the arguments given when we instantiated our Transcriber class.\n    \"\"\"\n    # Enumerate our input files.\n    for input_filename in sorted(self.filter.get_matching_files()):\n        if self.dry_run:\n            print(f\"DRY RUN ENABLED, skipping actual transcription of [{input_filename}]\")\n            continue\n        # Are we likely to overwrite an existing .srt file?\n        output_srt_file = input_filename.with_suffix(\".srt\")\n        if not self.force and output_srt_file.exists():\n            print(\n                f\"SKIPPING: Transcription for [{input_filename}] already exists \"\n                f\"as [{output_srt_file}] (use --force to overwrite).\"\n            )\n            continue\n\n        print(f\"PROCESSING: {input_filename} -&gt; {output_srt_file}...\")\n        transcription: dict[str, Any] | None = None\n        try:\n            transcription = self.transcribe(input_filename)\n        except IndexError as err:\n            print(f\"ERROR: Skipping [{input_filename}] due to [{err}]\")\n            continue\n        if transcription:\n            # Create a SubRipFile object to hold the subtitles.\n            subs = pysrt.SubRipFile()\n            for i, segment in enumerate(transcription[\"segments\"]):\n                start_time_ms = int(segment[\"start\"] * 1000)\n                end_time_ms = int(segment[\"end\"] * 1000)\n                text = segment[\"text\"].strip()\n\n                # Create SubRipTime objects.\n                start_time = pysrt.SubRipTime(milliseconds=start_time_ms)\n                end_time = pysrt.SubRipTime(milliseconds=end_time_ms)\n\n                # Create a SubRipItem and add it to the file.\n                sub = pysrt.SubRipItem(index=i + 1, start=start_time, end=end_time, text=text)\n                subs.append(sub)\n\n            # Save the SRT file.\n            subs.save(output_srt_file, encoding=\"utf-8\")\n\n            print(f\"SUCCESS: Transcription saved to [{output_srt_file}]\")\n        else:\n            print(f\"ERROR: Empty transcribe() return value: [{input_filename}]\")\n\n    print(\"Transcription completed for all files.\")\n</code></pre>"},{"location":"modules/#transcriber.transcribe.main","title":"<code>main(args=None)</code>","text":"<p>Main function to run the transcriber.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Optional[list[str]]</code> <p>List of command-line arguments to parse.</p> <code>None</code> Source code in <code>src/transcriber/transcribe.py</code> <pre><code>def main(args: list[str] | None = None) -&gt; None:\n    \"\"\"\n    Main function to run the transcriber.\n\n    Args:\n        args (Optional[list[str]]): List of command-line arguments to parse.\n    \"\"\"\n    # Parse command-line arguments, prompting if needed.\n    parsed_args: argparse.Namespace = parse_and_prompt_arguments(args)\n    # Create a Transcriber instance and run the transcription.\n    transcriber: Transcriber = Transcriber(parsed_args)\n    # Start the transcription process.\n    transcriber.videos_to_text()\n</code></pre>"},{"location":"modules/#transcriber.transcribe.parse_and_prompt_arguments","title":"<code>parse_and_prompt_arguments(args=None)</code>","text":"<p>Parse command-line arguments and prompt for a subset of missing ones if in interactive mode. We don't bother prompting for \"include\" or \"exclude\" arguments since we would encourage you to learn to use the command-line arguments for more advance usage.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>list[str] | None</code> <p>List of command-line arguments to parse.</p> <code>None</code> <p>Returns:</p> Type Description <code>argparse.Namespace</code> <p>The parsed command-line arguments.</p> <p>Raises:</p> Type Description <code>SystemExit</code> <p>If version is requested or invalid input is provided.</p> Source code in <code>src/transcriber/transcribe.py</code> <pre><code>def parse_and_prompt_arguments(args: list[str] | None = None) -&gt; argparse.Namespace:\n    \"\"\"\n    Parse command-line arguments and prompt for a subset of missing ones if in interactive mode.\n    We don't bother prompting for \"include\" or \"exclude\" arguments since we would encourage\n    you to learn to use the command-line arguments for more advance usage.\n\n    Args:\n        args: List of command-line arguments to parse.\n\n\n    Returns:\n        The parsed command-line arguments.\n\n    Raises:\n        SystemExit: If version is requested or invalid input is provided.\n\n    \"\"\"\n    # Define the full set of arguments\n    full_parser = argparse.ArgumentParser(description=\"Transcribe audio files using a pre-trained model.\")\n    full_parser.add_argument(\n        \"--dry-run\", \"-n\", action=\"store_true\", help=\"Try a dry run without any actual transcription.\"\n    )\n    full_parser.add_argument(\n        \"--include\",\n        type=str,\n        nargs=\"*\",\n        help=\"A list of files or rglob patterns to include when processing. Defaults to **/*.mp4.\",\n    )\n    full_parser.add_argument(\n        \"--exclude\",\n        type=str,\n        nargs=\"*\",\n        help=\"A list of files or rglob patterns to exclude from processing (overrides the include list).\",\n    )\n    full_parser.add_argument(\"--force\", action=\"store_true\", help=\"Force overwrite of existing output SRT files.\")\n    full_parser.add_argument(\n        \"--input-path\", type=str, help=\"Directory containing input audio files (required in non-interactive mode).\"\n    )\n    full_parser.add_argument(\n        \"--suffix\", type=validate_dot_suffix, default=\".mp4\", help=\"Suffix of audio files to process (default: .mp4).\"\n    )\n    # List of available Whisper models\n    english_only_models_list = sorted([model for model in whisper._MODELS if model.endswith(\".en\")])\n    english_only_models_str = \", \".join(english_only_models_list)\n    full_parser.add_argument(\n        \"--model\",\n        type=str,\n        default=\"base.en\",\n        choices=english_only_models_list,\n        help=f\"Pre-trained model to use (default: base.en, available {english_only_models_str}).\",\n    )\n    full_parser.add_argument(\n        \"--interactive\", action=\"store_true\", help=\"Run in interactive mode, prompting for missing arguments.\"\n    )\n    full_parser.add_argument(\"--version\", \"-v\", action=\"store_true\", help=\"Show program's version number and exit.\")\n\n    # First pass: Check for --interactive flag or no arguments\n    first_parser = argparse.ArgumentParser(add_help=False)\n    first_parser.add_argument(\"--interactive\", action=\"store_true\")\n    first_args, unknown_args = first_parser.parse_known_args(args)\n\n    # Case 1: No arguments supplied OR --interactive flag is present.\n    if not unknown_args or first_args.interactive:\n        # Parse arguments provided on the command line first\n        # This will set the values for any args that *were* provided,\n        # and leave others as their default or None.\n        parsed_args = full_parser.parse_args(args=unknown_args)\n        if parsed_args.version:\n            # Special handling for version in interactive mode.\n            print(f\"transcribe version: {__VERSION__}\")\n            sys.exit()\n\n        # Now enter interactive mode.\n        print(\"Entering interactive mode. Please provide the required information.\")\n\n        # Prompt for missing arguments.\n        if parsed_args.input_path is None:\n            input_path = input(\"Enter the directory with videos (default: .): \").strip() or \".\"\n            parsed_args.input_path = input_path\n\n        # Enumerate include/exclude patterns to remove empties and duplicates.\n        parsed_args.exclude = sorted({pattern for pattern in parsed_args.exclude or [] if pattern})\n        parsed_args.include = sorted({pattern for pattern in parsed_args.include or [] if pattern})\n\n        # The other arguments have defaults, but you can still ask for\n        # confirmation or allow changes.\n        print(f\"\\nCurrent settings for transcribe version {__VERSION__}:\")\n        print(f\"  Input Path: {parsed_args.input_path}\")\n        print(f\"  Suffix: {parsed_args.suffix}\")\n        print(f\"  Model: {parsed_args.model}\")\n        print(f\"  Force overwrite: {'Yes' if parsed_args.force else 'No'}\")\n        print(f\"  Dry run: {'Yes' if parsed_args.dry_run else 'No'}\")\n        print(f\"  Excluded patterns: ({', '.join(parsed_args.exclude) if parsed_args.exclude else 'None'})\")\n        print(f\"  Include patterns: ({', '.join(parsed_args.exclude) if parsed_args.include else 'None'})\")\n        print(\"\\nYou will now be prompted for any changes to these settings.\")\n\n        # Prompt for changes to defaulted arguments.\n        # Ask the user if they want to change the suffix.\n        suffix = input(f\"Enter suffix to process (or press Enter to keep '{parsed_args.suffix}'): \").strip()\n        # No suffix given, use our default.\n        if not suffix:\n            suffix = \".mp4\"\n        parsed_args.suffix = validate_dot_suffix(suffix)\n        # Ask the user if they want to change the Whisper model.\n        model = input(\n            f\"Enter model to use (or press Enter to keep '{parsed_args.model}', available {english_only_models_str}): \"\n        ).strip()\n        parsed_args.model = model or \"base.en\"\n        # Perform validation on interactive model input, ignoring case.\n        if parsed_args.model not in english_only_models_list:\n            print(\"Invalid model selected. Exiting...\")\n            sys.exit(1)  # Barf...\n        # Ask the user if they want to force overwriting of existing SRT files.\n        force = (\n            input(f\"Force overwrite of existing SRT files? (y/N, default: {'Y' if parsed_args.force else 'N'}): \")\n            .strip()\n            .lower()\n        )\n        if force == \"y\":\n            parsed_args.force = True\n        # Ask the user if they want to perform a \"dry run\" where no SRT files are written.\n        dry_run = input(f\"Enable dry run mode? (y/N, default: {'Y' if parsed_args.dry_run else 'N'}): \").strip().lower()\n        if dry_run == \"y\":\n            parsed_args.dry_run = True\n\n        # Confirm the user's changes.\n        print(f\"\\nConfirm settings for transcribe version {__VERSION__}:\")\n        print(f\"  Suffix: {parsed_args.suffix}\")\n        print(f\"  Model: {parsed_args.model}\")\n        print(f\"  Force overwrite: {'Yes' if parsed_args.force else 'No'}\")\n        print(f\"  Dry run: {'Yes' if parsed_args.dry_run else 'No'}\")\n        print(f\"  Excluded patterns: ({', '.join(parsed_args.exclude) if parsed_args.exclude else 'None'})\")\n        print(f\"  Include patterns: ({', '.join(parsed_args.exclude) if parsed_args.include else 'None'})\")\n        print(\"\\nHit Enter to continue, or Ctrl-C to abort.\")\n        input()\n\n        return parsed_args\n\n    else:\n        # Case 2: Arguments supplied, and not interactive mode.\n        parsed_args = full_parser.parse_args(unknown_args)\n        # Special handling for version in non-interactive mode.\n        if parsed_args.version:\n            print(f\"transcribe version: {__VERSION__}\")\n            sys.exit()\n        return parsed_args\n</code></pre>"},{"location":"modules/#transcriber.transcribe.validate_dot_suffix","title":"<code>validate_dot_suffix(value)</code>","text":"<p>A custom argparse type that ensures the value is a string starting with a dot '.'.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The input string to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The validated suffix string.</p> <p>Raises:</p> Type Description <code>argparse.ArgumentTypeError</code> <p>If the value is invalid.</p> Source code in <code>src/transcriber/transcribe.py</code> <pre><code>def validate_dot_suffix(value: str) -&gt; str:\n    \"\"\"\n    A custom argparse type that ensures the value is a string\n    starting with a dot '.'.\n\n    Args:\n        value: The input string to validate.\n\n    Returns:\n        The validated suffix string.\n\n    Raises:\n        argparse.ArgumentTypeError: If the value is invalid.\n    \"\"\"\n    if not isinstance(value, str) or not value.startswith(\".\"):\n        # This specific exception is caught by argparse and printed\n        # to the user as a clean error message.\n        print(f\"invalid suffix: '{value}' (must start with a '.')\")\n        raise argparse.ArgumentTypeError()\n    return value\n</code></pre>"},{"location":"coverage/","title":"Coverage","text":""}]}